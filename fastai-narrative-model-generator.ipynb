{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV'S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are loading the CSV files generated from the previous web scraping notebook. We will see some cleaning up in the below cells. The generation and exporting of the CSV files from the previous notebook caused some of these additional elements to be injected. Be sure to look at the dataframe first to see if these items have been inserted. The Fastai won't be able to run properly, or at all, if they're in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files'\n",
    "# be sure to change this path if you're doing it on your own device. You can do this by importing and running below\n",
    "# import os\n",
    "# os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files/dfNarrative.csv')\n",
    "# # df.pop('Unnamed: 0.1')\n",
    "# df.pop('Unnamed: 0')\n",
    "# df.dropna()\n",
    "# df = df.dropna()\n",
    "df.to_csv('/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files/dfNarrative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files/dfDialogue.csv')\n",
    "# # df.pop('Unnamed: 0.1')\n",
    "# df.pop('Unnamed: 0')\n",
    "# df.dropna()\n",
    "# df = df.dropna()\n",
    "df.to_csv('/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files/dfDialogue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.words.replace(regex=True,inplace=Tre,to_replace=[r'\\t', 'â', '\\(\\)' ],value=r'')\n",
    "# this element is left over from the previous notebook. After looking at the data, it may need to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>“My dear Mr Bennet,” said his lady to him one day, “have you heard that      Netherfield Park is let at last?”</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“But it is,” returned she; “for Mrs Long has just been here, and she told      me all about it”</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“Do you not want to know who has taken it?” cried his wife impatiently</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“ want to tell me, and I have no objection to hearing it”</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“Why, my dear, you must know, Mrs Long says that Netherfield is taken by      a young man of large fortune from the north of England; that he came down      on Monday in a chaise and four to see the place, and was so much delighted      with it, that he agreed with Mr Morris immediately; that he is to take      possession before Michaelmas, and some of his servants are to be in the      house by the end of next week”</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>For a long moment no one spoke Then Paulette, with tears of happinessdimming her eyes, turned and gripped Adam's hand in her own \"Mydear,\" she smiled, \"thank you\" As Adam turned to her, he felt Waylandgripping his other hand tightly \"I understand, sir Nothing will besaid\" Adam smiled tenderly at Paulette, then his eyes turned anxiouslyto search those of his crew On every face was a commending grin ofapproval In every pair of eyes was a promise that had been given, andwould be kept With a suspicious huskiness in his voice, Adam drewhimself erect \"Thank you gentlemen,\" he said softly The crew filedout</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>With his arm around Paulette, he drew her gently to the starboard port,and pointed to a dim, fast-receding, silver-green orb \"There it is,darling I don't know whether to curse it or bless it\" He grinned ather quizzically She came close to him, and her arms stole gently abouthim \"I'll bless it as long as I live,\" she breathed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74913 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [      “My dear Mr Bennet,” said his lady to him one day, “have you heard that      Netherfield Park is let at last?”     ,       “But it is,” returned she; “for Mrs Long has just been here, and she told      me all about it”     ,       “Do you not want to know who has taken it?” cried his wife impatiently    ,       “ want to tell me, and I have no objection to hearing it”     ,       “Why, my dear, you must know, Mrs Long says that Netherfield is taken by      a young man of large fortune from the north of England; that he came down      on Monday in a chaise and four to see the place, and was so much delighted      with it, that he agreed with Mr Morris immediately; that he is to take      possession before Michaelmas, and some of his servants are to be in the      house by the end of next week”     ,       “What is his name?”     ,       “Is he married or single?”     ,       “Oh! Single, my dear, to be sure! A single man of large fortune; four or      five thousand a year What a fine thing for our girls!”     ,       “How so? How can it affect them?”     ,       “My dear Mr Bennet,” replied his wife, “how can you be so tiresome! You      must know that I am thinking of his marrying one of them”     ,       “Is that his design in settling here?”     ,       “Design! Nonsense, how can you talk so! But it is very likely that he       fall in love with one of them, and therefore you must visit him as soon as      he comes”     ,       “I see no occasion for that You and the girls may go, or you may send      them by themselves, which perhaps will be still better, for as you are as      handsome as any of them, Mr Bingley may like you the best of the party”     ,       “My dear, you flatter me I certainly  had my share of beauty,      but I do not pretend to be anything extraordinary now When a woman has      five grown-up daughters, she ought to give over thinking of her own      beauty”     ,       “In such cases, a woman has not often much beauty to think of”     ,       “But, my dear, you must indeed go and see Mr Bingley when he comes into      the neighbourhood”     ,       “It is more than I engage for, I assure you”     ,       “But consider your daughters Only think what an establishment it would be      for one of them Sir William and Lady Lucas are determined to go, merely      on that account, for in general, you know, they visit no newcomers Indeed      you must go, for it will be impossible for  to visit him if you      do not”     ,       “You are over-scrupulous, surely I dare say Mr Bingley will be very glad      to see you; and I will send a few lines by you to assure him of my hearty      consent to his marrying whichever he chooses of the girls; though I must      throw in a good word for my little Lizzy”     ,       “I desire you will do no such thing Lizzy is not a bit better than the      others; and I am sure she is not half so handsome as Jane, nor half so      good-humoured as Lydia But you are always giving  the      preference”     ,       “They have none of them much to recommend them,” replied he; “they are all      silly and ignorant like other girls; but Lizzy has something more of      quickness than her sisters”     ,       “Mr Bennet, how can you abuse your own children in such a way? You      take delight in vexing me You have no compassion for my poor nerves”     ,       “You mistake me, my dear I have a high respect for your nerves They are      my old friends I have heard you mention them with consideration these      last twenty years at least”     ,       “Ah, you do not know what I suffer”     ,       “But I hope you will get over it, and live to see many young men of four      thousand a year come into the neighbourhood”     ,       “It will be no use to us, if twenty such should come, since you will not      visit them”     ,       “Depend upon it, my dear, that when there are twenty, I will visit them      all”     ,       “I hope Mr Bingley will like it, Lizzy”     ,       “We are not in a way to know  Mr Bingley likes,” said her      mother resentfully, “since we are not to visit”     ,       “But you forget, mamma,” said Elizabeth, “that we shall meet him at the      assemblies, and that Mrs Long promised to introduce him”     ,       “I do not believe Mrs Long will do any such thing She has two nieces of      her own She is a selfish, hypocritical woman, and I have no opinion of      her”     ,       “No more have I,” said Mr Bennet; “and I am glad to find that you do not      depend on her serving you”     ,       “Don’t keep coughing so, Kitty, for Heaven’s sake! Have a little      compassion on my nerves You tear them to pieces”     ,       “Kitty has no discretion in her coughs,” said her father; “she times them      ill”     ,       “I do not cough for my own amusement,” replied Kitty fretfully “When is      your next ball to be, Lizzy?”     ,       “To-morrow fortnight”     ,       “Aye, so it is,” cried her mother, “and Mrs Long does not come back till      the day before; so it will be impossible for her to introduce him, for she      will not know him herself”     ,       “Then, my dear, you may have the advantage of your friend, and introduce      Mr Bingley to ”     ,       “Impossible, Mr Bennet, impossible, when I am not acquainted with him      myself; how can you be so teasing?”     ,       “I honour your circumspection A fortnight’s acquaintance is certainly      very little One cannot know what a man really is by the end of a      fortnight But if  do not venture somebody else will; and after      all, Mrs Long and her nieces must stand their chance; and, therefore,      as she will think it an act of kindness, if you decline the office, I will      take it on myself”     ,       The girls stared at their father Mrs Bennet said only, “Nonsense,      nonsense!”     ,       “What can be the meaning of that emphatic exclamation?” cried he “Do you      consider the forms of introduction, and the stress that is laid on them,      as nonsense? I cannot quite agree with you  What say you,      Mary? For you are a young lady of deep reflection, I know, and read great      books and make extracts”     ,       “While Mary is adjusting her ideas,” he continued, “let us return to Mr      Bingley”     ,       “I am sick of Mr Bingley,” cried his wife    ,       “I am sorry to hear ; but why did not you tell me that before?      If I had known as much this morning I certainly would not have called on      him It is very unlucky; but as I have actually paid the visit, we cannot      escape the acquaintance now”     ,       “How good it was in you, my dear Mr Bennet! But I knew I should persuade      you at last I was sure you loved your girls too well to neglect such an      acquaintance Well, how pleased I am! and it is such a good joke, too,      that you should have gone this morning and never said a word about it till      now”     ,       “Now, Kitty, you may cough as much as you choose,” said Mr Bennet; and,      as he spoke, he left the room, fatigued with the raptures of his wife    ,       “What an excellent father you have, girls!” said she, when the door was      shut “I do not know how you will ever make him amends for his kindness;      or me, either, for that matter At our time of life it is not so pleasant,      I can tell you, to be making new acquaintances every day; but for your      sakes, we would do anything Lydia, my love, though you  the      youngest, I dare say Mr Bingley will dance with you at the next ball”     ,       “Oh!” said Lydia stoutly, “I am not afraid; for though I  the      youngest, I’m the tallest”     ,       “If I can but see one of my daughters happily settled at Netherfield,”       said Mrs Bennet to her husband, “and all the others equally well married,      I shall have nothing to wish for”     ,       “Come, Darcy,” said he, “I must have you dance I hate to see you standing      about by yourself in this stupid manner You had much better dance”     ,       “I certainly shall not You know how I detest it, unless I am particularly      acquainted with my partner At such an assembly as this it would be      insupportable Your sisters are engaged, and there is not another woman in      the room whom it would not be a punishment to me to stand up with”     ,       “I would not be so fastidious as you are,” cried Mr Bingley, “for a      kingdom! Upon my honour, I never met with so many pleasant girls in my      life as I have this evening; and there are several of them you see      uncommonly pretty”     ,       “ are dancing with the only handsome girl in the room,” said Mr      Darcy, looking at the eldest Miss Bennet    ,       “Oh! She is the most beautiful creature I ever beheld! But there is one of      her sisters sitting down just behind you, who is very pretty, and I dare      say very agreeable Do let me ask my partner to introduce you”     ,       “Which do you mean?” and turning round he looked for a moment at      Elizabeth, till catching her eye, he withdrew his own and coldly said:      “She is tolerable, but not handsome enough to tempt ; I am in no      humour at present to give consequence to young ladies who are slighted by      other men You had better return to your partner and enjoy her smiles, for      you are wasting your time with me”     ,       “If he had had any compassion for ,” cried her husband      impatiently, “he would not have danced half so much! For God’s sake, say      no more of his partners Oh that he had sprained his ankle in the first      dance!”     ,       “Oh! my dear, I am quite delighted with him He is so excessively      handsome! And his sisters are charming women I never in my life saw      anything more elegant than their dresses I dare say the lace upon Mrs      Hurst’s gown—”     ,       “But I can assure you,” she added, “that Lizzy does not lose much by not      suiting  fancy; for he is a most disagreeable, horrid man, not      at all worth pleasing So high and so conceited that there was no enduring      him! He walked here, and he walked there, fancying himself so very great!      Not handsome enough to dance with! I wish you had been there, my dear, to      have given him one of your set-downs I quite detest the man”     ,       “He is just what a young man ought to be,” said she, “sensible,      good-humoured, lively; and I never saw such happy manners!—so much      ease, with such perfect good breeding!”     ,       “He is also handsome,” replied Elizabeth, “which a young man ought      likewise to be, if he possibly can His character is thereby complete”     ,       “I was very much flattered by his asking me to dance a second time I did      not expect such a compliment”     ,       “Oh! you are a great deal too apt, you know, to like people in general      You never see a fault in anybody All the world are good and agreeable in      your eyes I never heard you speak ill of a human being in your life”     ,       “I would not wish to be hasty in censuring anyone; but I always speak what      I think”     ,       “Certainly not—at first But they are very pleasing women when you      converse with them Miss Bingley is to live with her brother, and keep his      house; and I am much mistaken if we shall not find a very charming      neighbour in her”     ,       “Yes; but he seemed to like his second better”     ,       “Perhaps you mean what I overheard between him and Mr Robinson; did not I      mention it to you? Mr Robinson’s asking him how he liked our Meryton      assemblies, and whether he did not think there were a great many pretty      women in the room, and  he thought the prettiest? and his      answering immediately to the last question: ‘Oh! the eldest Miss Bennet,      beyond a doubt; there cannot be two opinions on that point’”     ,       “Upon my word! Well, that is very decided indeed—that does seem as      if—but, however, it may all come to nothing, you know”     ,       “I beg you would not put it into Lizzy’s head to be vexed by his      ill-treatment, for he is such a disagreeable man, that it would be quite a      misfortune to be liked by him Mrs Long told me last night that he sat      close to her for half-an-hour without once opening his lips”     ,       “Are you quite sure, ma’am?—is not there a little mistake?” said      Jane “I certainly saw Mr Darcy speaking to her”     ,       “Aye—because she asked him at last how he liked Netherfield, and he      could not help answering her; but she said he seemed quite angry at being      spoke to”     ,       “Miss Bingley told me,” said Jane, “that he never speaks much, unless      among his intimate acquaintances With  he is remarkably      agreeable”     ,       “I do not believe a word of it, my dear If he had been so very agreeable,      he would have talked to Mrs Long But I can guess how it was; everybody      says that he is eat up with pride, and I dare say he had heard somehow      that Mrs Long does not keep a carriage, and had come to the ball in a      hack chaise”     ,       “I do not mind his not talking to Mrs Long,” said Miss Lucas, “but I wish      he had danced with Eliza”     ,       “Another time, Lizzy,” said her mother, “I would not dance with ,      if I were you”     ,       “I believe, ma’am, I may safely promise you  to dance with      him”     ,       “Pride,” observed Mary, who piqued herself upon the solidity of her      reflections, “is a very common failing, I believe By all that I have ever      read, I am convinced that it is very common indeed; that human nature is      particularly prone to it, and that there are very few of us who do not      cherish a feeling of self-complacency on the score of some quality or      other, real or imaginary Vanity and pride are different things, though      the words are often used synonymously A person may be proud without being      vain Pride relates more to our opinion of ourselves, vanity to what we      would have others think of us”     ,       “If I were as rich as Mr Darcy,” cried a young Lucas, who came with his      sisters, “I should not care how proud I was I would keep a pack of      foxhounds, and drink a bottle of wine a day”     ,       “Then you would drink a great deal more than you ought,” said Mrs Bennet;      “and if I were to see you at it, I should take away your bottle directly”     ,       “But she does help him on, as much as her nature will allow If       can perceive her regard for him, he must be a simpleton, indeed,      not to discover it too”     ,       “Remember, Eliza, that he does not know Jane’s disposition as you do”     ,       “But if a woman is partial to a man, and does not endeavour to conceal it,      he must find it out”     ,       “Perhaps he must, if he sees enough of her But, though Bingley and Jane      meet tolerably often, it is never for many hours together; and, as they      always see each other in large mixed parties, it is impossible that every      moment should be employed in conversing together Jane should therefore      make the most of every half-hour in which she can command his attention      When she is secure of him, there will be more leisure for falling in love      as much as she chooses”     ,       “Your plan is a good one,” replied Elizabeth, “where nothing is in      question but the desire of being well married, and if I were determined to      get a rich husband, or any husband, I dare say I should adopt it But      these are not Jane’s feelings; she is not acting by design As yet, she      cannot even be certain of the degree of her own regard nor of its      reasonableness She has known him only a fortnight She danced four dances      with him at Meryton; she saw him one morning at his own house, and has      since dined with him in company four times This is not quite enough to      make her understand his character”     ,       “Not as you represent it Had she merely  with him, she might      only have discovered whether he had a good appetite; but you must remember      that four evenings have also been spent together—and four evenings      may do a great deal”     ,       “Yes; these four evenings have enabled them to ascertain that they both      like Vingt-un better than Commerce; but with respect to any other leading      characteristic, I do not imagine that much has been unfolded”     ,       “Well,” said Charlotte, “I wish Jane success with all my heart; and if she      were married to him to-morrow, I should think she had as good a chance of      happiness as if she were to be studying his character for a twelvemonth      Happiness in marriage is entirely a matter of chance If the dispositions      of the parties are ever so well known to each other or ever so similar      beforehand, it does not advance their felicity in the least They always      continue to grow sufficiently unlike afterwards to have their share of      vexation; and it is better to know as little as possible of the defects of      the person with whom you are to pass your life”     ,       “You make me laugh, Charlotte; but it is not sound You know it is not      sound, and that you would never act in this way yourself”     ,       “What does Mr Darcy mean,” said she to Charlotte, “by listening to my      conversation with Colonel Forster?”     ,       “That is a question which Mr Darcy only can answer”     ,       “But if he does it any more I shall certainly let him know that I see what      he is about He has a very satirical eye, and if I do not begin by being      impertinent myself, I shall soon grow afraid of him”     ,       “Did you not think, Mr Darcy, that I expressed myself uncommonly well      just now, when I was teasing Colonel Forster to give us a ball at      Meryton?”     ,       “With great energy; but it is always a subject which makes a lady      energetic”     ,       “You are severe on us”     ,       “It will be  turn soon to be teased,” said Miss Lucas “I am      going to open the instrument, Eliza, and you know what follows”     ,       “You are a very strange creature by way of a friend!—always wanting      me to play and sing before anybody and everybody! If my vanity had taken a      musical turn, you would have been invaluable; but as it is, I would really      rather not sit down before those who must be in the habit of hearing the      very best performers” On Miss Lucas’s persevering, however, she added,      “Very well, if it must be so, it must” And gravely glancing at Mr Darcy,      “There is a fine old saying, which everybody here is of course familiar      with: ‘Keep your breath to cool your porridge’; and I shall keep mine to      swell my song”     ,       “What a charming amusement for young people this is, Mr Darcy! There is      nothing like dancing after all I consider it as one of the first      refinements of polished society”     ,       “Certainly, sir; and it has the advantage also of being in vogue amongst      the less polished societies of the world Every savage can dance”     ,       Sir William only smiled “Your friend performs delightfully,” he continued      after a pause, on seeing Bingley join the group; “and I doubt not that you      are an adept in the science yourself, Mr Darcy”     ,       “You saw me dance at Meryton, I believe, sir”     , ...]\n",
       "\n",
       "[74913 rows x 0 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('/home/jupyter/tutorials/fastai/course-v3/nbs/dl1/book-files/dfDialogue.csv', index_col='words')\n",
    "df.fillna('')\n",
    "df.tail(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the NaN elements are the items that need to be removed by running the above cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed from the Fastai course, we can take a closer look at what's going on under the hood:\n",
    "\n",
    "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation. such as our commas, periods...etc...\n",
    "- some words are contractions of two different words, like isn't or don't So our 's, 't and 're can be classified as proper conjunctions...etc...\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance or other random artifacts\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/fastai/data_block.py:537: UserWarning: You are labelling your items with CategoryList.\n",
      "Your valid set contained the following unknown labels, the corresponding items have been discarded.\n",
      "94847, 40578, 47246, 11450, 67689...\n",
      "  if getattr(ds, 'warn', False): warn(ds.warn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos that was a relief wherever you be let your wind go free who knows if that xxunk i took with my cup of tea after was quite good with the heat i xxunk anything off it i m sure that xxunk man in the xxunk is agreat rogue i hope that lamp is not smoking fill my nose up with xxunk betterthan having him leaving the gas on all</td>\n",
       "      <td>33284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos 1 . xxmaj it is grossly false that our ministers , as is said in a note , had proposed to surrender our claims to compensation for xxmaj spanish spoliations , or even for xxmaj french . xxmaj their instructions were to make no treaty in which xxmaj spanish spoliations were not provided for ; and although they were permitted to be silent as to xxmaj french spoliations carried</td>\n",
       "      <td>20721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj the indocile liberty of this member is very remarkable , so xxunk unruly in its tumidity and impatience , when we do not require it , and so unseasonably disobedient , when we stand most in need of it : so imperiously contesting in authority with the will , and with so much haughty obstinacy denying all solicitation , both of hand and mind . xxmaj and yet</td>\n",
       "      <td>55374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i must now trouble xxmaj congress with a petition on my own behalf . xxmaj when i left my own house in xxmaj october , 1783 , it was to attend xxmaj congress as a member , and in expectation of returning in five or six months . xxmaj in the month of xxmaj may following , however , i was desired to come to xxmaj europe , as</td>\n",
       "      <td>18682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj the mass of human concerns , moral and physical , is so vast , the field of knowledge requisite for man to conduct them to the best advantage is so extensive , that no human being can acquire the whole himself , and much less in that degree necessary for the instruction of others . xxmaj it has of necessity , then , been distributed into different departments</td>\n",
       "      <td>21139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TextClasDataBunch.from_csv(path, 'dfNarrative.csv')\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like this: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________ ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TextClasDataBunch?\n",
    "This class method calls upon a parent class of DataLoader. This is how Fastai will prepare our specific text data for use in recurrent neural networks. DataLoader is the base framework for machine learning preparation. TextClasDataBunch is one of the several methods used to prepare data. Others include classes for vision learning and tabular learning (csv type data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is DataLoader?\n",
    "Taking a closer look under the hood, we can see that DataLoader is the Pytorch component of Fastai. This is essentially combines our dataset with a sampler. so it will iterate through our data and help arrange it in the fashion best suited for whichever machine learning element we are trying to accomplish. In this case, we're using Natural Language Processing (NLP). \n",
    "To put it simply, we are determining what size of chunks we want to split our data into, the randomness of those chunks, how many samples we want and then turning those bits of data into Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Tensor?\n",
    "To put it simply, a tensor is a saved portion of memory that is stored in its linear algebra form. This is the backbone of machine learning. By storing it in this linear form, we are able to take these chunks of data and process them through our graphics cards. This is what makes machine learning more practical. The speed of running our weights and activations via linear algebra processes allows us to process results magnitudes above what the CPU could provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is DataBunch?\n",
    "DataBunch is the Fastai method of grouping together our data loaded via DataLoader. It's easier to consider that DataLoader is the 'under the hood' portion of our NLP, and DataBunch is closer to a user level component. \n",
    "Here we are essentially taking our data that was provided to DataLoader and splitting it between several groups. More specifically, these groups are going to be training sets, validations sets and test sets. This is where our subsequent Fastai functions will be pulling the information from to conduct the machine learning.\n",
    "So to put it all together:\n",
    "- TextDataBunch (or it's other derivatives) calls upon DataLoader to take the provided data and transform it into a usable interface for the linear functions used by the graphics card processor\n",
    "- DataLoader prepares the provided data in a Tensor form\n",
    "- DataBunch takes the DataLoader product and places it into validation, train and test folders in order to utilize the further Fastai functions to conduct machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondence from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.vocab.itos[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can lo\n",
    "# data.train_ds[0][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_ds[0][0].data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the data block API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
    "\n",
    "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're relevant, so it'll be more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here. This will break down our data into digestable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book literature is in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.\n",
    "\n",
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because written English novels aren't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the novels dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = (TextList.from_csv(path, 'dfNarrative.csv', cols=1)\n",
    "           #Inputs: all the text files in path\n",
    "#             .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .split_by_rand_pct(0.1)\n",
    "           #We randomly split and keep 10% of our book segments for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs))\n",
    "data_lm.save('data_lmNarrative.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextList:\n",
    "Let's take a closer look at what's going on above. At first glance, it looks similar to what we did in previous cells when we created our 'data' object. However, in this case we will be utilizing those items created by the function.\n",
    "This process is setting up our tokenized items and splitting them into their proper training, validation and optionally test sets. We further indicate that these items need to be labeled in preparation for our language model learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere). This will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set). And then it will send batches that read that text in an attempt to aim for targets that are the next word in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cell above being a bit long, we want to load quickly the final ids by using the following cell.\n",
    "data_lm = load_data(path, 'data_lmNarrative.pkl', bs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>property of some one or other of their daughters . xxbos xxmaj mr. xxmaj bennet made no answer . xxbos xxmaj this was invitation enough . xxbos xxmaj mr. xxmaj bennet was so odd a mixture of quick parts , sarcastic humour , reserve , and caprice , that the experience of three - and - twenty years had been insufficient to make his wife understand his character . mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the general gaze from the foundations of the world — the figure of the sharp female called xxmaj la xxmaj guillotine . xxbos xxmaj it was the popular theme for jests ; it was the best cure for headache , it infallibly prevented the hair from turning grey , it imparted a peculiar delicacy to the complexion , it was the xxmaj national xxmaj razor which shaved close : who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>churchill , was only growing to dislike him more . xxmaj he began to suspect him of some double dealing in his pursuit of xxmaj emma . xxmaj that xxmaj emma was his object appeared indisputable . xxmaj every thing declared it ; his own attentions , his father 's hints , his mother - in - law 's guarded silence ; it was all in unison ; words ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>average for our xxmaj unit . xxmaj this is a work proper to be committed to mathematicians as well as merchants , and which should be decided on actual and accurate experiment . xxbos xxmaj the quantum of alloy is also to be decided . xxmaj some is necessary , to prevent the coin from wearing too fast ; too much , fills our pockets with copper , instead of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>, to the heads of departments the trouble of making up , once a day , a packet of all their communications for the perusal of the xxmaj president ; it commonly also retarded one day their despatches by mail . xxmaj but in pressing cases , this injury was prevented by presenting that case singly for immediate attention ; and it produced us in return the benefit of his</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `the directory pathway` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the language_model_learner?\n",
    "Here we have another class method that is using our data_lm object in addition to two other elements involving a 'language learner.' Let's take a closer look at this methodology.\n",
    "\n",
    "The learner in this case is 'AWD_LSTM.' This is an acronym for \" Average (Stochastic Gradient Descent) Weight Dropped- Long-Short Term Memory Networks.\" This is understably a mouth full. But it's important to understand the Stochastic Gradient Descent (SGD) is the bedrock of machine learning. Everything we have been previously doing is preparing the data to perform this function. \n",
    "The AWD_LSTM is also known as the arch type. In this case, that means it also utilizes the previously mentioned [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset) backbone. As mentioned before, it was trained to predict the next word in a sentence after given a certain arrangement of words (in this case, it was words in wikipedia pages). We will refer back to this in the future, as it involves utilizing the previously taught (learned) weights and allows us to remove the back end to build on top of it.\n",
    "\n",
    "This is an extensive topic that requires more than a snippet of a Jupyter Notebook markdown, however, you can find detailed understandings via fast.ai's online course. I will do my best to give the general sense of its functionality.\n",
    "_____\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD):\n",
    "This is an iterative optimization algorithm, where in the objective is to find a set of parameters that produces the minimum value in a given set of inputs. In our case for machine learning, we are trying to find the best parameters of an unknown function to produce the minimum amount of incorrect answers produces by our machine learning program. \n",
    "\n",
    "There are of course several components involved in SGD. Such as; Momentum, Averaging and Adaptive Movement Estimation along with several others. We will eventually go over these items as we dig into the code.\n",
    "\n",
    "#### Weight Dropping:\n",
    "\n",
    "In general the AWD_LSTM will utilize the weight dropout method during the layers of the Recurrent Neural Network (RNN) and it's subsequent final activation layer. Why Randomized dropping of weights? Simply put, it has been empirically proven to work. The function is basically saving a copy of a weight before deleting it, then running through a round of the full RNN. The program can then compare these outcomes to other rounds of iterating through the RNN to find a more optimal solution.\n",
    "\n",
    "Doing this through several epochs (rounds through all of the layers of the RNN) has proven to provide a much higher accuracy for machine learning predictions. This methodology applies to all aspects of machine learning and not just Natural Language Processing.\n",
    "__________\n",
    "\n",
    "Finally, we can see that our last input to the class is 'drop_mult.' This is simply the amount of random weight drops we wish to see within our learner.\n",
    "\n",
    "- There are other defaults within the source code, however, we won't jump into them now. They're essentially options regarding when and where to apply the weight dropping previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_headDialogue');\n",
    "#NOTE THIS METHOD IS DEPRICATED FOR VERSIONS PAST v1.0.43. it is simply loading the previously saved items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='97' class='' max='7206', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.35% [97/7206 00:08<10:43 15.0540]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Finder:\n",
    "This is our learning rate finder. What is a learning rate? In generic terms, it's the rate at which we alter our Stochastic Gradient Descent function to find the lowest outcome. Again, I will generally explain what's occurring but I recommend further reading on your part to gain a better understanding.\n",
    "\n",
    "In the general understanding of the SGD, we assume the parabola to be smooth from top to bottom, to top. However, we empirically know this is not the case. There are local minima and maxima within the parabola. So, instead of smooth it is generally more mountainous. This leads to a few negative outcomes.\n",
    "\n",
    "- Example of 'Ideal' SDG:\n",
    "<img src=\"https://miro.medium.com/max/1005/1*_6TVU8yGpXNYDkkpOfnJ6Q.png\" alt=\"Stochastic vs Batch Gradient Descent - Divakar Kapil - Medium\" style=\"width: 450px; height: 279.403px; margin: 0px;\"/>\n",
    "(Source from medium.com)\n",
    "\n",
    "\n",
    "- Reality of SDG:\n",
    "<img src=\"https://imgs.developpaper.com/imgs/bprop.png\" alt=\"Summary of Varieties of Gradient Decline Method | Develop Paper\" style=\"width: 400px; height: 300px; margin: 0px;\"/>\n",
    "\n",
    "\n",
    "One common problem, we could have a learning rate that is too small. This could cause our alogorithm to become stuck in a local minima that is actually not the optimal solution. However, the machine doesn't know that because we forced it to give us a failed solution through our methodology.\n",
    "\n",
    "Another common problem is having our learing rate set too high. This could unintentionally cause us to jump too far ahead of the optimal solution and essentially ping pong back and forth accomplishing nothing. Fortunately, the smart folks' at fastai have several tested solutions for this issue. We will get into these specifics in a few cells. For now, Let's take a look at the 'lr_find' function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr_find:\n",
    "\n",
    "This class method actually calls upon another class referred to as LRFinder. In this case, it causes a mock learning session that pulls 100 test learning rates between the range of 1e-07 and 10. Why do this? It's so we can avoid some of the pitfalls mentioned above. We don't want to start training too high and we also don't want to start too low. This gives us a tried and practiced method of finding a good learning rate starter point for future functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=15) #we are simply taking a plot of the loss calculated using our 100 plotting points\n",
    "# we generated in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.685467</td>\n",
       "      <td>4.485846</td>\n",
       "      <td>0.248819</td>\n",
       "      <td>11:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_one_cycle:\n",
    "Here we have our first look at actually machine learning in action, with the fit_one_cycle function. There's a lot to unpack here. Let's get a glimpse of the code within the function;\n",
    "\n",
    "- fit_one_cycle(learn:Learner, cyc_len:int, max_lr:Union[float, Collection[float], slice]=slice(None, 0.003, None), moms:Point=(0.95, 0.85), div_factor:float=25.0, pct_start:float=0.3, final_div:float=None, wd:float=None, callbacks:Optional[Collection[Callback]]=None, tot_epochs:int=None, start_epoch:int=None)\n",
    "\n",
    "Here we can see where our first two elements of '1' and '1e-2' come into play in the previous cell. Which corresponds to a cycle length (cyc_len) of 1 and a maximum learning rate (max_lr) of 1e-2. \n",
    "\n",
    "The first portion is fairly sensible, but what about the learning rate? In this case, we are actually determining how high we will allow the learning rate to go. Remember from the previous description of lr_find? We actually found our learning rate in the graph above. We took one order of magnitude prior to the learing rate going exponentially out of control. \n",
    "\n",
    "Let's take a look at the third element in the function, 'moms.' I highlighted previously that our learning rate determines how easily we can use our SGD algorithm to find our most optimal answer (the lowest amount of loss). However, having one static number is not the current methodology. Through practical testing, it shows that using momentum in the computations provides a faster time to optimization. How? It calculates a weight to add towards the forward computations. This puts more emphasis on weights that are higher (and more likely to correspond to the lower loss outcome) and puts minimal change on small weights that have a minimal impact on results of the answers produce by the machine learning function.\n",
    "\n",
    "As you can see from the above fit_one_cycle function, there are several more default elements within. Here's a quick glimpse of the first few:\n",
    "\n",
    "- div_factor:\n",
    "Here we use a determined dividing factor to calculate how much we need to increase/decrease our learning rate by, and then how much we decrease and increase the momentum by. To clarify, we have two different components happening simultaneously. \n",
    "First, we are taking our lr_max (which was 1e-2 in this case) and dividing that by our div_factor (25 in this case). Taking this computation, we are starting from a starting learning rate (0.003 as indicated by our max_lr:Union) and adding our result to it. We will then add this amount on each iteration until the maximum learning rate is hit (which is 0.02 as we indicated in the above cell). At the same time, we are decreasing our momentum from our maximum momentum rate(0.8) to our lowest momentum rate (0.7) by scaled steps based on the previous calculation from the div_factor.\n",
    "\n",
    "Second, once we hit the maximum learning rate, and subsequent minimum momentum, we will reverse course and now subtract the calculated div_factor and add back on our momentum. This will be done until we hit our respective minimum learning rate and maximum momentum rate. Then there's one more step.\n",
    "\n",
    "Lastly, we will take the max_lr/div_factor and multiply it by 100. This will further decrease our learning rate while keeping our maximum momentum the same.\n",
    "\n",
    "Here is a visual example:\n",
    "\n",
    "<img src=\"https://docs.fast.ai/imgs/onecycle_params.png\" alt=\"1cycle parameteres\" style=\"width: 1000px; height: 279.403px; margin: 0px;\"/>\n",
    "(Sourced from fast.ai)\n",
    "\n",
    "\n",
    "Why do this? Again, it has been empirically tested. This process was modeled after a technique referred to as 'annealing.' Where in the physical sense, it is used to polarize physical objects by heating them up, and cooling them down over periods of time. In the sense of machine learning, this heating, cooling and amount of pressure over time will more effectively navigate around the previously mentioned pitfalls of the SDG (such as too low learning rates, too high, getting stuck in local minimas and then adding too much momentum to explode out of the SDG).\n",
    "\n",
    "- Example of learning rates:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*7nmYLhx4cZ_gLPXiyiy6vQ.png\" alt=\"FAST AI JOURNEY: PART 1. LESSON 3. THEORY REVIEW. LEARNING RATES ...\" style=\"width: 800px; height: 300px; margin: 10.8px 0px;\"/>\n",
    "(Sourced from medium.com)\n",
    "\n",
    "- pct_start:\n",
    "This one is referring to the amount of iterations we actually want the learning rate to change for. Instead of going up one step per iteration, we will randomly select a percentage that will add towards (or subtract from) the learning rates.\n",
    "\n",
    "- There are other elements within the function, however, we won't go into details. They are optional items that will be used later on, such as weight decay, callbacks and start_epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_headNarrative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_headNarrative');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unfreeze:\n",
    "This function is worth noting in order to further explain the use of architecture backbones (AWD-LSTM in this case) is applied. When we first developed the 'learn' object we identified the backbone AWD-LSTM as our previously trained model. Then we utilized 'fit_one_cycle.' When we did this, all but the last of the layers in the previous model (the wikitext-103 layers within AWD-LSTM) were utlized from the previous training. The final activation layer was replaced with another round of training based upon our data. And then our own final activation layer was inserted after that.\n",
    "\n",
    "Now when we utilize the unfreeze function, we are allowing all of the other layers within the wikitext-103 to modify their weights along with the portion we added towards the end. This bedrock of machine learning is referred to as transfer learning.\n",
    "\n",
    "### Transfer Learning:\n",
    "This is such an important concept towards machine learning, that it's worth mentioning twice. \n",
    "Here is one definition;\n",
    "\n",
    "- Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n",
    "\n",
    "The above explanation of unfreeze also provides a explanation of how transfer learning can be used in practice. In that particular case, we used the previously learned knowledge of wikitext-103 and applied it to our book literature data. In other words, someone else took the time to teach a model how to predict the next word in a wikipedia page. We took all of those efforts and then applied them to learn how to predict the next word in our literature data. And there we have it, transfer machine learning in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.00% [8/10 1:32:01<23:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.277780</td>\n",
       "      <td>4.221658</td>\n",
       "      <td>0.269095</td>\n",
       "      <td>11:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.279351</td>\n",
       "      <td>4.215765</td>\n",
       "      <td>0.275184</td>\n",
       "      <td>11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.246326</td>\n",
       "      <td>4.199677</td>\n",
       "      <td>0.279409</td>\n",
       "      <td>11:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.198082</td>\n",
       "      <td>4.169220</td>\n",
       "      <td>0.283250</td>\n",
       "      <td>11:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.101477</td>\n",
       "      <td>4.137623</td>\n",
       "      <td>0.287488</td>\n",
       "      <td>11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.007540</td>\n",
       "      <td>4.116825</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>11:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.955282</td>\n",
       "      <td>4.106951</td>\n",
       "      <td>0.292166</td>\n",
       "      <td>11:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.789952</td>\n",
       "      <td>4.106759</td>\n",
       "      <td>0.293250</td>\n",
       "      <td>11:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3541' class='' max='7206', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      49.14% [3541/7206 05:23<05:35 3.7610]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the same process we used in our previous fit_one_cycle function. The difference here is that we requested 10 epochs to be performed. We can now also delve into what our numbers are that have generated for each epoch:\n",
    "\n",
    "#### train_loss / valid_loss:\n",
    "This is simply the loss function calculation performed after the last activation function in our recurrent neural network. One is for our training set (the 90% portion of the data we set apart) and our validation set (the other 10% of our data).\n",
    "There are actually a variety of ways this loss function can be calculated, we won't go into those minute details here, but know that a basic default used in fast.ai is the cross-entropy loss function. Again, it is a tried and true method to help us minimize the loss function and increase our accuracy.\n",
    "\n",
    "#### Accuracy:\n",
    "This is essentially a generic calculation from our loss functions that tells us how accurate our model is at predicting the next word in our literature examples. Our highest is about 29.3%. It may sound low, but let's think about it. Out of all of the words available in our literature AND wikipedia examples, our model can accurately predict the next word almost 1/3 of the time. That may be on par with most humans. In the future, we can try to see where our accuracy is failing at. However, it's a good guess that the complexity of written narration (involving several forms of vernacular across several time periods) is causing learning troubles.\n",
    "\n",
    "#### Time:\n",
    "This is simply the amount of time it takes to run through 1 epoch of the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "You may see the above error message if you let your computer go into sleep mode. I had personally been running my data processing on google cloud platform (please see the [fast.ai](https://course.fast.ai/start_gcp.html) page to learn how to set one up yourself). It's highly recommended considering they give a very generous credit towards computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tunedNarrative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tunedDialogue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = '\"Is it the Coronavirus? I asked.\"'\n",
    "N_WORDS = 150\n",
    "N_SENTENCES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Is it the Coronavirus? I asked.\" xxbos “ Well : you must not be angry : i have not had enough of your company to say so But it is no time for me to go into the drawing - room , and i have not a moment to talk of it ” xxbos “ i have no idea , ” replied Monte Cristo ; “ but , unfortunately , you have made me a present of five thousand ” xxbos “ That is true , ” answered Sancho , “ but i will go to my own house , for i have a firm heart , and i am willing to take my oath ” xxbos “ What ! ” he cried , “ do you think that this Frenchman has a right to make a ball ? ” xxbos “ Then i will tell you\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.7) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn.predict:\n",
    "Above is a nice built in function that allows us to use our taught model to predict words based off of our inputs. In this case I used the object containing the phrase \"Is it the Coronavirus? I asked.\" This then prompted our model to predict a certain amount of words and sentences we determined. In this case, I had it predict 150 words in the form of 1 sentence, which is a bit silly but it gives us some interesting combinations.\n",
    "\n",
    "Each time the function is ran, it will generate different words. Feel free to play around with it.\n",
    "\n",
    "I would also like to note that this is just one aspect we can perform with our model. We can also train our model to predict sentiment of the writing and classify it into groups that we have determined. There is even other architecture backbones to utilize for this purpose. If you are interested in performing these i recommend checking out the [fast.ai github page](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-imdb.ipynb) notebook that shows this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_encDialogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
